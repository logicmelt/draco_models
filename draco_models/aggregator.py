import scipy.stats
import numpy as np
from typing import Any
from collections import Counter


class Aggregator(object):

    def __init__(self):
        """
        Initialize the Aggregator class.
        This class will be used to aggregate data from an InfluxDB instance covering the desired time range.

        See the documentation for more detailes about the keys in the data dictionary.
        """
        super().__init__()

    def aggregate(self, data: dict[str, Any]) -> dict[str, Any]:
        """Aggregate the data. This method estimates the mean, stdev, kurtosis and skewness of several parameters.

        Args:
            data (dict[str, Any]): Data from an influx database.

        Returns:
            dict[str, Any]: A dictionary containing the average multiplicity and the standard deviation of the multiplicity.
        """
        output_aggregate: dict[str, Any] = {}

        output_aggregate["multiplicity"] = self.get_multiplicity(data).item()

        # Get the mean, standard deviation, kurtosis and skewness of the azimuthal angle
        output_aggregate["mean_azimuth"] = np.mean(data["phi[rad]"]).item()
        output_aggregate["variance_azimuth"] = np.std(data["phi[rad]"], ddof=1).item()
        output_aggregate["skewness_azimuth"] = scipy.stats.skew(data["phi[rad]"]).item()
        output_aggregate["kurtosis_azimuth"] = scipy.stats.kurtosis(
            data["phi[rad]"]
        ).item()

        # Get the mean, standard deviation, kurtosis and skewness of the zenithal angle
        output_aggregate["mean_zenith"] = np.mean(data["theta[rad]"]).item()
        output_aggregate["variance_zenith"] = np.std(data["theta[rad]"], ddof=1).item()
        output_aggregate["skewness_zenith"] = scipy.stats.skew(
            data["theta[rad]"]
        ).item()
        output_aggregate["kurtosis_zenith"] = scipy.stats.kurtosis(
            data["theta[rad]"]
        ).item()

        # Get the total number of readings
        output_aggregate["n_readings"] = len(data["EventID"])

        # And last, the density_day_idx so that we can know which density profile was used
        output_aggregate["density_day_idx"] = np.unique(
            data["density_day_idx"]
        ).tolist()

        return output_aggregate

    def get_multiplicity(self, data: dict[str, Any]) -> np.floating:
        """Get the average multiplicity of the particles.

        Args:
            data (dict[str, Any]): Data from an influx database including "EventID" and "process_ID" keys.

        Returns:
            np.float64: The average multiplicity of the particles.
        """
        # We need two columns: EventID and process_ID
        # If EventID is the same then it was generated by the same primary but we have to check that the process_ID is also the same
        eventid = np.array(data["EventID"])
        processid = np.array(data["process_ID"])
        # Get the unique process_ID
        unique_process = np.unique(processid)
        # Now iterate and get the multiplicity of each process
        multiplicity: list[int] = []
        for process in unique_process:
            # Get the indexes of the process
            indexes = np.where(processid == process)[0]
            iter_multi = Counter(eventid[indexes])
            multiplicity.extend(iter_multi.values())

        return np.mean(multiplicity)
